# Evaluation & Quality Assurance Implementation Handoff

## Overview

This document tracks the implementation of **Evaluation & Quality Assurance** competencies for the jama-mcp-server-graphrag project.

## Phase Status

| Phase | Description | Status | PR |
|-------|-------------|--------|-----|
| **Phase 1** | Benchmark Suite & Datasets | ðŸŸ¢ Complete | Pending |
| **Phase 2** | Custom Domain Metrics | â¬œ Not Started | â€” |
| **Phase 3** | CI/CD Integration | â¬œ Not Started | â€” |
| **Phase 4** | MLflow Comparison | â¬œ Not Started | â€” |
| **Phase 5** | Cost/Token Tracking | â¬œ Not Started | â€” |
| **Phase 6** | Human Feedback Loop | â¬œ Not Started | â€” |

---

## Phase 1: Benchmark Suite (Complete)

### Files Created

```
tests/benchmark/
â”œâ”€â”€ __init__.py                    âœ… Package exports
â”œâ”€â”€ schemas.py                     âœ… Pydantic models (BenchmarkExample, etc.)
â”œâ”€â”€ templates.py                   âœ… Query templates + domain concepts
â”œâ”€â”€ generator.py                   âœ… Programmatic dataset generation
â”œâ”€â”€ golden_dataset.py              âœ… 30 hand-curated critical examples
â”œâ”€â”€ conftest.py                    âœ… Pytest fixtures
â”œâ”€â”€ test_retrieval_accuracy.py     âœ… Retrieval quality tests
â”œâ”€â”€ test_answer_quality.py         âœ… Answer generation tests
â”œâ”€â”€ test_agentic_routing.py        âœ… Router decision tests
â””â”€â”€ test_latency_performance.py    âœ… Performance benchmarks

scripts/
â””â”€â”€ generate_benchmark_dataset.py  âœ… CLI for dataset generation
```

### Key Components

#### Schemas (`schemas.py`)
- `QueryCategory`: DEFINITIONAL, RELATIONAL, PROCEDURAL, COMPARISON, FACTUAL, ANALYTICAL, EDGE_CASE
- `DifficultyLevel`: EASY, MEDIUM, HARD, EXPERT
- `ExpectedRouting`: All MCP tools mapped
- `BenchmarkExample`: Complete example with metadata

#### Golden Dataset (`golden_dataset.py`)
- 30 hand-curated examples
- Covers all query categories
- Includes critical edge cases (out-of-domain, typos, multi-part)
- All tagged as `must-pass` for CI

#### Generator (`generator.py`)
- Programmatic generation from templates
- Combines 8 template sets Ã— 25+ domain concepts
- Produces 250+ diverse examples
- Deterministic with seed parameter

### Verification Commands

```bash
# Run benchmark tests
uv run pytest tests/benchmark/ -v

# Generate dataset statistics
uv run python scripts/generate_benchmark_dataset.py --stats-only

# Generate and save dataset
uv run python scripts/generate_benchmark_dataset.py --output benchmark_data.json

# Generate LangSmith-compatible format
uv run python scripts/generate_benchmark_dataset.py --langsmith-format --output langsmith_eval.json

# View golden dataset only
uv run python scripts/generate_benchmark_dataset.py --golden-only
```

### Test Coverage

| Test File | Test Count | Description |
|-----------|------------|-------------|
| `test_retrieval_accuracy.py` | ~20 | Precision@K, Recall@K, MRR |
| `test_answer_quality.py` | ~20 | Faithfulness, relevancy checks |
| `test_agentic_routing.py` | ~15 | Tool selection accuracy |
| `test_latency_performance.py` | ~15 | Latency thresholds, throughput |

---

## Phase 2: Custom Domain Metrics (Planned)

### Deliverables

```
src/jama_mcp_server_graphrag/evaluation/
â””â”€â”€ domain_metrics.py    # NEW
```

### Metrics to Implement

| Metric | Description | Formula |
|--------|-------------|---------|
| Citation Accuracy | Standards correctly cited | correct_citations / total_citations |
| Traceability Coverage | Links mentioned when relevant | traced_refs / expected_refs |
| Technical Precision | Domain terms used correctly | correct_terms / total_terms |
| Completeness Score | All aspects of query addressed | aspects_covered / aspects_asked |
| Regulatory Alignment | ISO/ASPICE/FDA refs accurate | aligned_refs / regulatory_refs |

---

## Phase 3: CI/CD Integration (Planned)

### CI Tiers

| Tier | Trigger | Scope | Time | Cost |
|------|---------|-------|------|------|
| 1 | Every PR | Unit tests, prompt validation | ~1 min | $0 |
| 2 | Merge to main | Smoke eval (10 queries) | ~5 min | ~$0.50 |
| 3 | Release tag | Full benchmark (250 queries) | ~20 min | ~$15 |
| 4 | Nightly | Deep eval + A/B tests | ~45 min | ~$20 |

### Files to Create/Modify

```
.github/workflows/
â”œâ”€â”€ ci.yml               # MODIFY: Add Tier 1-2
â””â”€â”€ evaluation.yml       # NEW: Tier 3-4

scripts/
â””â”€â”€ ci_evaluation.py     # NEW: CI-friendly runner
```

---

## Phase 4: MLflow Comparison (Planned)

### Deliverables

```
src/jama_mcp_server_graphrag/
â”œâ”€â”€ mlflow_tracking.py           # NEW
â””â”€â”€ observability_comparison.py  # NEW

docs/
â””â”€â”€ PLATFORM_COMPARISON.md       # NEW

scripts/
â””â”€â”€ compare_platforms.py         # NEW
```

### Comparison Dimensions

1. Setup complexity
2. Evaluation features
3. Visualization
4. Prompt versioning
5. Self-hosting options

---

## Phase 5: Cost/Token Tracking (Planned)

### Cost Thresholds

```python
COST_THRESHOLDS = {
    "query_budget_target": 0.015,
    "query_budget_warning": 0.025,
    "query_budget_alert": 0.040,
    "query_budget_hard_limit": 0.100,
}
```

### Deliverables

```
src/jama_mcp_server_graphrag/evaluation/
â””â”€â”€ cost_metrics.py      # NEW

src/jama_mcp_server_graphrag/
â””â”€â”€ token_counter.py     # NEW
```

---

## Phase 6: Human Feedback Loop (Planned)

### Workflow

1. Export low-confidence runs to annotation queues
2. Human reviewers annotate
3. Import annotations as evaluation examples
4. Re-run evaluations

### Deliverables

```
docs/
â””â”€â”€ FEEDBACK_WORKFLOW.md         # NEW

scripts/
â”œâ”€â”€ export_for_annotation.py     # NEW
â”œâ”€â”€ import_feedback.py           # NEW
â””â”€â”€ update_datasets.py           # NEW
```

---

## Quick Reference

### Run All Benchmark Tests
```bash
uv run pytest tests/benchmark/ -v
```

### Run Specific Category
```bash
uv run pytest tests/benchmark/test_retrieval_accuracy.py -v
```

### Generate Full Dataset
```bash
uv run python scripts/generate_benchmark_dataset.py \
    --count 250 \
    --include-golden \
    --langsmith-format \
    --output eval_dataset.json
```

### Check Dataset Statistics
```bash
uv run python scripts/generate_benchmark_dataset.py --stats-only
```

---

## Dependencies Added

None - all dependencies already in pyproject.toml:
- pytest
- pytest-asyncio
- pytest-benchmark (optional, for performance tracking)

---

## Next Steps

1. **Verify Phase 1**: Run tests, check for issues
2. **Create PR**: Commit Phase 1 changes
3. **Start Phase 2**: Implement domain metrics
4. **Iterate**: Complete remaining phases

---

## Contact

This handoff document enables continuation in a new Claude session. Reference this file along with the project's SPECIFICATION.md and CLAUDE.md for full context.
