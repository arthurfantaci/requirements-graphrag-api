# Continuous Integration Pipeline
name: CI

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  PYTHON_VERSION: "3.12"

jobs:
  lint:
    name: Lint & Format Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Run Ruff linter
        run: uv run ruff check . --output-format=github

      - name: Run Ruff formatter check
        run: uv run ruff format --check .

  test:
    name: Test
    runs-on: ubuntu-latest
    needs: [lint]
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Run tests with coverage
        run: uv run pytest --cov=jama_mcp_server_graphrag --cov-report=xml --cov-report=html
        env:
          # Use test values for unit tests (mocked)
          NEO4J_URI: neo4j://localhost:7687
          NEO4J_USERNAME: neo4j
          NEO4J_PASSWORD: test
          OPENAI_API_KEY: sk-test

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          file: ./coverage.xml
          fail_ci_if_error: false

      - name: Upload coverage HTML report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: htmlcov/

  build-docker:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: [test]
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: false
          load: true
          tags: jama-mcp-server-graphrag:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Test Docker image
        run: |
          docker run --rm -d --name test-container \
            -e NEO4J_URI=neo4j://localhost:7687 \
            -e NEO4J_USERNAME=neo4j \
            -e NEO4J_PASSWORD=test \
            -e OPENAI_API_KEY=sk-test \
            jama-mcp-server-graphrag:${{ github.sha }} \
            sleep 30

          # Wait for container to start
          sleep 5

          # Verify Python imports work
          docker exec test-container python -c "from jama_mcp_server_graphrag import server; print('Import OK')"

          # Cleanup
          docker stop test-container

  # Tier 1: Prompt validation (runs on every PR)
  prompt-validation:
    name: Tier 1 - Prompt Validation
    runs-on: ubuntu-latest
    needs: [lint]
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Run prompt validation (Tier 1)
        run: uv run python scripts/ci_evaluation.py --tier 1 --output tier1_results.json
        env:
          NEO4J_URI: neo4j://localhost:7687
          NEO4J_USERNAME: neo4j
          NEO4J_PASSWORD: test

      - name: Upload Tier 1 results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: tier1-evaluation-results
          path: tier1_results.json

  # Integration tests run on main branch only with real credentials
  integration-test:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [test]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Run integration tests
        # Exit code 5 means no tests collected, which is OK if no integration tests exist yet
        run: |
          uv run pytest -m integration --tb=short || exit_code=$?
          if [ "${exit_code:-0}" -eq 5 ]; then
            echo "No integration tests found (exit code 5) - this is OK"
            exit 0
          fi
          exit ${exit_code:-0}
        env:
          NEO4J_URI: ${{ secrets.NEO4J_URI }}
          NEO4J_USERNAME: ${{ secrets.NEO4J_USERNAME }}
          NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD }}
          NEO4J_DATABASE: ${{ secrets.NEO4J_DATABASE }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

  # Tier 2: Smoke evaluation (runs on merge to main)
  smoke-evaluation:
    name: Tier 2 - Smoke Evaluation
    runs-on: ubuntu-latest
    needs: [test, prompt-validation]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Run smoke evaluation (Tier 2)
        run: uv run python scripts/ci_evaluation.py --tier 2 --output tier2_results.json
        env:
          NEO4J_URI: ${{ secrets.NEO4J_URI }}
          NEO4J_USERNAME: ${{ secrets.NEO4J_USERNAME }}
          NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD }}
          NEO4J_DATABASE: ${{ secrets.NEO4J_DATABASE }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
          LANGSMITH_PROJECT: jama-graphrag-ci
          LANGSMITH_TRACING: "true"

      - name: Upload Tier 2 results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: tier2-evaluation-results
          path: tier2_results.json
