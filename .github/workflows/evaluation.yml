# Per-Vector Evaluation Pipeline
# - Tier 2: Smoke test (manual only, ~$0.50)
# - Tier 3: Full benchmark on release tags (v*) or manual (~$15)
# - Tier 4: Deep evaluation (manual only, ~$20)
name: Evaluation

# Grant permissions for creating issues and commit comments
permissions:
  contents: write
  issues: write

on:
  # Tier 3: Release tags
  push:
    tags:
      - 'v*'

  # Manual trigger with tier selection
  workflow_dispatch:
    inputs:
      tier:
        description: 'Evaluation tier to run'
        required: true
        default: '3'
        type: choice
        options:
          - '2'
          - '3'
          - '4'

env:
  PYTHON_VERSION: "3.12"

jobs:
  # Tier 2: Smoke Test (manual only)
  smoke-test:
    name: Tier 2 - Smoke Test
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.tier == '2'
    timeout-minutes: 10
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Run smoke test (Tier 2)
        run: uv run python scripts/ci_evaluation.py --tier 2 --output tier2_results.json --verbose
        env:
          NEO4J_URI: ${{ secrets.NEO4J_URI }}
          NEO4J_USERNAME: ${{ secrets.NEO4J_USERNAME }}
          NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD }}
          NEO4J_DATABASE: ${{ secrets.NEO4J_DATABASE }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
          LANGSMITH_PROJECT: graphrag-api-smoke
          LANGSMITH_TRACING: "true"

      - name: Upload Tier 2 results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: tier2-smoke-results-${{ github.sha }}
          path: backend/tier2_results.json

  # Tier 3: Full Benchmark (release tags or manual)
  full-benchmark:
    name: Tier 3 - Full Benchmark
    runs-on: ubuntu-latest
    if: >-
      (github.event_name == 'push' && startsWith(github.ref, 'refs/tags/v')) ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.tier == '3')
    timeout-minutes: 30
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Run full benchmark (Tier 3)
        run: uv run python scripts/ci_evaluation.py --tier 3 --regression-gate --output tier3_results.json --verbose
        env:
          NEO4J_URI: ${{ secrets.NEO4J_URI }}
          NEO4J_USERNAME: ${{ secrets.NEO4J_USERNAME }}
          NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD }}
          NEO4J_DATABASE: ${{ secrets.NEO4J_DATABASE }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
          LANGSMITH_PROJECT: graphrag-api-release
          LANGSMITH_TRACING: "true"

      - name: Upload Tier 3 results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: tier3-benchmark-results-${{ github.ref_name || github.sha }}
          path: backend/tier3_results.json

      - name: Comment on release with results
        if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags/v')
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const results = JSON.parse(fs.readFileSync('backend/tier3_results.json', 'utf8'));

            let vectorTable = '';
            for (const [vector, data] of Object.entries(results.vectors)) {
              vectorTable += `\n### ${vector} (${data.sample_count} samples)\n\n`;
              vectorTable += `| Metric | Score |\n|--------|-------|\n`;
              for (const [metric, score] of Object.entries(data.scores).sort()) {
                vectorTable += `| ${metric} | ${Number(score).toFixed(4)} |\n`;
              }
            }

            const regression = results.regression
              ? `\n**Regression Gate**: ${results.regression.passed ? 'PASSED' : 'FAILED'}`
              : '';

            const body = [
              '## Release Evaluation Results',
              '',
              `**Tier 3 - Full Benchmark**: ${results.passed ? 'PASSED' : 'FAILED'}`,
              `**Duration**: ${results.duration_seconds}s`,
              vectorTable,
              regression,
              results.errors && results.errors.length > 0
                ? '\n**Errors:**\n' + results.errors.map(e => `- ${e}`).join('\n')
                : '',
            ].join('\n');

            await github.rest.repos.createCommitComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              commit_sha: context.sha,
              body: body,
            });

  # Tier 4: Deep Evaluation (manual only)
  deep-evaluation:
    name: Tier 4 - Deep Evaluation
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.tier == '4'
    timeout-minutes: 60
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Run deep evaluation (Tier 4)
        run: uv run python scripts/ci_evaluation.py --tier 4 --regression-gate --output tier4_results.json --verbose
        env:
          NEO4J_URI: ${{ secrets.NEO4J_URI }}
          NEO4J_USERNAME: ${{ secrets.NEO4J_USERNAME }}
          NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD }}
          NEO4J_DATABASE: ${{ secrets.NEO4J_DATABASE }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
          LANGSMITH_PROJECT: graphrag-api-deep
          LANGSMITH_TRACING: "true"

      - name: Upload Tier 4 results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: tier4-deep-eval-${{ github.run_number }}
          path: backend/tier4_results.json

      - name: Create issue on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let resultsInfo = 'Results file not available';

            try {
              const results = JSON.parse(fs.readFileSync('backend/tier4_results.json', 'utf8'));

              let vectorSummary = '';
              for (const [vector, data] of Object.entries(results.vectors || {})) {
                vectorSummary += `\n- **${vector}** (${data.sample_count} samples): `;
                vectorSummary += Object.entries(data.scores)
                  .map(([m, s]) => `${m}=${Number(s).toFixed(3)}`)
                  .join(', ');
              }

              const regression = results.regression
                ? `\n- Regression gate: ${results.regression.passed ? 'PASSED' : 'FAILED'}`
                : '';

              resultsInfo = `${vectorSummary}${regression}\n- Errors: ${(results.errors || []).join(', ') || 'None'}`;
            } catch (e) {
              resultsInfo = `Could not read results: ${e.message}`;
            }

            const issueTitle = `[Bug]: Deep Evaluation Failed - ${new Date().toISOString().split('T')[0]}`;
            const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;

            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: issueTitle,
              body: [
                '## Description',
                '',
                'The Tier 4 deep evaluation failed during a manual run.',
                '',
                `**Workflow Run:** [#${context.runNumber}](${runUrl})`,
                '',
                '## Results',
                '',
                resultsInfo,
              ].join('\n'),
              labels: ['evaluation', 'bug', 'automated'],
              assignees: ['arthurfantaci']
            });

  # Summary job to aggregate results
  evaluation-summary:
    name: Evaluation Summary
    runs-on: ubuntu-latest
    needs: [smoke-test, full-benchmark, deep-evaluation]
    if: always() && (needs.smoke-test.result != 'skipped' || needs.full-benchmark.result != 'skipped' || needs.deep-evaluation.result != 'skipped')
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: results

      - name: Generate summary
        run: |
          echo "# Evaluation Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          for f in results/*/tier*_results.json; do
            if [ -f "$f" ]; then
              echo "## $(basename $(dirname $f))" >> $GITHUB_STEP_SUMMARY
              echo '```json' >> $GITHUB_STEP_SUMMARY
              cat "$f" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          done
