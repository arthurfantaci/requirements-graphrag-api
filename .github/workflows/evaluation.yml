# Evaluation Pipeline for Tier 3-4
# - Tier 3: Full benchmark on release tags (v*)
# - Tier 4: Deep evaluation nightly
name: Evaluation

# Grant permissions for creating issues and commit comments
permissions:
  contents: write
  issues: write

on:
  # Tier 3: Release tags
  push:
    tags:
      - 'v*'

  # Tier 4: Nightly schedule (2 AM UTC)
  schedule:
    - cron: '0 2 * * *'

  # Manual trigger with tier selection
  workflow_dispatch:
    inputs:
      tier:
        description: 'Evaluation tier to run'
        required: true
        default: '3'
        type: choice
        options:
          - '3'
          - '4'

env:
  PYTHON_VERSION: "3.12"

jobs:
  # Tier 3: Full Benchmark (release)
  full-benchmark:
    name: Tier 3 - Full Benchmark
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || (github.event_name == 'workflow_dispatch' && github.event.inputs.tier == '3')
    timeout-minutes: 30
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Run full benchmark (Tier 3)
        run: uv run python scripts/ci_evaluation.py --tier 3 --output tier3_results.json --verbose
        env:
          NEO4J_URI: ${{ secrets.NEO4J_URI }}
          NEO4J_USERNAME: ${{ secrets.NEO4J_USERNAME }}
          NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD }}
          NEO4J_DATABASE: ${{ secrets.NEO4J_DATABASE }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
          LANGSMITH_PROJECT: graphrag-api-release
          LANGSMITH_TRACING: "true"

      - name: Upload Tier 3 results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: tier3-benchmark-results-${{ github.ref_name || github.sha }}
          path: backend/tier3_results.json

      - name: Comment on release with results
        if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags/v')
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const results = JSON.parse(fs.readFileSync('backend/tier3_results.json', 'utf8'));

            const body = `## ðŸ“Š Release Evaluation Results

            **Tier 3 - Full Benchmark**

            | Metric | Value |
            |--------|-------|
            | Status | ${results.passed ? 'âœ… PASSED' : 'âŒ FAILED'} |
            | Samples Evaluated | ${results.samples_evaluated} |
            | Average Score | ${results.avg_score.toFixed(4)} |
            | Threshold | ${results.min_score_threshold.toFixed(4)} |
            | Duration | ${results.duration_seconds.toFixed(1)}s |
            | Est. Cost | $${results.cost_estimate.toFixed(2)} |

            ${results.errors.length > 0 ? '\n**Errors:**\n' + results.errors.map(e => `- ${e}`).join('\n') : ''}
            `;

            // Post results as a commit comment on the tagged commit
            await github.rest.repos.createCommitComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              commit_sha: context.sha,
              body: body,
            });

  # Tier 4: Deep Evaluation (nightly)
  deep-evaluation:
    name: Tier 4 - Deep Evaluation
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.tier == '4')
    timeout-minutes: 60
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --extra dev

      - name: Run deep evaluation (Tier 4)
        run: uv run python scripts/ci_evaluation.py --tier 4 --output tier4_results.json --verbose
        env:
          NEO4J_URI: ${{ secrets.NEO4J_URI }}
          NEO4J_USERNAME: ${{ secrets.NEO4J_USERNAME }}
          NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD }}
          NEO4J_DATABASE: ${{ secrets.NEO4J_DATABASE }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
          LANGSMITH_PROJECT: graphrag-api-nightly
          LANGSMITH_TRACING: "true"

      - name: Upload Tier 4 results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: tier4-deep-eval-${{ github.run_number }}
          path: backend/tier4_results.json

      - name: Create issue on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let resultsInfo = 'Results file not available';

            try {
              const results = JSON.parse(fs.readFileSync('backend/tier4_results.json', 'utf8'));
              resultsInfo = `
              - Average Score: ${results.avg_score.toFixed(4)}
              - Threshold: ${results.min_score_threshold.toFixed(4)}
              - Samples: ${results.samples_evaluated}
              - Errors: ${results.errors.join(', ') || 'None'}
              `;
            } catch (e) {
              resultsInfo = `Could not read results: ${e.message}`;
            }

            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `[Bug]: Nightly Evaluation Failed - ${new Date().toISOString().split('T')[0]}`,
              body: `## Description

            The Tier 4 deep evaluation failed during the nightly run.

            ## Steps to Reproduce

            This issue was auto-generated by the nightly evaluation workflow.

            **Workflow Run:** [#${context.runNumber}](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})

            ## Expected Behavior

            All evaluation samples should pass with an average score at or above the configured threshold.

            ## Actual Behavior

            ${resultsInfo}

            ## Component

            Backend API

            ## Additional Context

            Please investigate and fix any regressions.

            /cc @${context.actor}
            `,
              labels: ['evaluation', 'bug', 'automated'],
              assignees: ['arthurfantaci']
            });

  # Summary job to aggregate results
  evaluation-summary:
    name: Evaluation Summary
    runs-on: ubuntu-latest
    needs: [full-benchmark, deep-evaluation]
    if: always() && (needs.full-benchmark.result != 'skipped' || needs.deep-evaluation.result != 'skipped')
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: results

      - name: Generate summary
        run: |
          echo "# Evaluation Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          for f in results/*/tier*_results.json; do
            if [ -f "$f" ]; then
              echo "## $(basename $(dirname $f))" >> $GITHUB_STEP_SUMMARY
              echo '```json' >> $GITHUB_STEP_SUMMARY
              cat "$f" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          done
